{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim uses Python’s standard logging module to log various stuff at various priority levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-13 10:55:14,499 : INFO : 'pattern' package not found; tag filters are not available for English\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#9 docs(9 rows) and 12 features ([0, to 11, ])\n",
    "corpus = [[(0, 1.0), (1, 1.0), (2, 1.0)],\n",
    " [(2, 1.0), (3, 1.0), (4, 1.0), (5, 1.0), (6, 1.0), (8, 1.0)],\n",
    " [(1, 1.0), (3, 1.0), (4, 1.0), (7, 1.0)],\n",
    " [(0, 1.0), (4, 2.0), (7, 1.0)],\n",
    " [(3, 1.0), (5, 1.0), (6, 1.0)],\n",
    " [(9, 1.0)],\n",
    " [(9, 1.0), (10, 1.0)],\n",
    " [(9, 1.0), (10, 1.0), (11, 1.0)],\n",
    "  [(8, 1.0), (10, 1.0), (11, 1.0)]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-13 13:42:00,138 : INFO : collecting document frequencies\n",
      "2017-04-13 13:42:00,140 : INFO : PROGRESS: processing document #0\n",
      "2017-04-13 13:42:00,165 : INFO : calculating IDF weights for 9 documents and 11 features (28 matrix non-zeros)\n"
     ]
    }
   ],
   "source": [
    "# implement the trasnformation\n",
    "tfidf = models.TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.8075244024440723), (4, 0.5898341626740045)]\n"
     ]
    }
   ],
   "source": [
    "vec = [(0, 1), (4, 1)]\n",
    "print(tfidf[vec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-13 13:42:37,761 : INFO : creating sparse index\n",
      "2017-04-13 13:42:37,816 : INFO : creating sparse matrix from corpus\n",
      "2017-04-13 13:42:37,851 : INFO : PROGRESS: at document #0\n",
      "2017-04-13 13:42:38,485 : INFO : created <9x12 sparse matrix of type '<class 'numpy.float32'>'\n",
      "\twith 28 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "# To transform the whole corpus via TfIdf and index it, in preparation for similarity queries:\n",
    "index = similarities.SparseMatrixSimilarity(tfidf[corpus], num_features=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to query the similarity of our query vector vec against every document in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.4662244), (1, 0.19139354), (2, 0.24600551), (3, 0.82094586), (4, 0.0), (5, 0.0), (6, 0.0), (7, 0.0), (8, 0.0)]\n"
     ]
    }
   ],
   "source": [
    "sims = index[tfidf[vec]]\n",
    "print(list(enumerate(sims)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpora and Vector Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = [\"Human machine interface for lab abc computer applications\",\n",
    ">>>              \"A survey of user opinion of computer system response time\",\n",
    ">>>              \"The EPS user interface management system\",\n",
    ">>>              \"System and human system engineering testing of EPS\",\n",
    ">>>              \"Relation of user perceived response time to error measurement\",\n",
    ">>>              \"The generation of random binary unordered trees\",\n",
    ">>>              \"The intersection graph of paths in trees\",\n",
    ">>>              \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    ">>>              \"Graph minors A survey\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['human', 'interface', 'computer'],\n",
      " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
      " ['eps', 'user', 'interface', 'system'],\n",
      " ['system', 'human', 'system', 'eps'],\n",
      " ['user', 'response', 'time'],\n",
      " ['trees'],\n",
      " ['graph', 'trees'],\n",
      " ['graph', 'minors', 'trees'],\n",
      " ['graph', 'minors', 'survey']]\n"
     ]
    }
   ],
   "source": [
    ">>> # remove common words and tokenize\n",
    ">>> stoplist = set('for a of the and to in'.split())\n",
    ">>> texts = [[word for word in document.lower().split() if word not in stoplist]\n",
    ">>>          for document in documents]\n",
    ">>>\n",
    ">>> # remove words that appear only once\n",
    ">>> from collections import defaultdict\n",
    ">>> frequency = defaultdict(int)\n",
    ">>> for text in texts:\n",
    ">>>     for token in text:\n",
    ">>>         frequency[token] += 1\n",
    ">>>\n",
    ">>> texts = [[token for token in text if frequency[token] > 1]\n",
    ">>>          for text in texts]\n",
    ">>>\n",
    ">>> from pprint import pprint  # pretty-printer\n",
    ">>> pprint(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-13 14:55:45,501 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2017-04-13 14:55:45,519 : INFO : built Dictionary(12 unique tokens: ['response', 'system', 'minors', 'computer', 'time']...) from 9 documents (total 29 corpus positions)\n",
      "2017-04-13 14:55:45,543 : INFO : saving Dictionary object under /tmp/deerwester.dict, separately None\n",
      "2017-04-13 14:55:45,911 : INFO : saved /tmp/deerwester.dict\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(12 unique tokens: ['response', 'system', 'minors', 'computer', 'time']...)\n"
     ]
    }
   ],
   "source": [
    ">>> dictionary = corpora.Dictionary(texts)\n",
    ">>> dictionary.save('/tmp/deerwester.dict')  # store the dictionary, for future reference\n",
    ">>> print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 5, 'system': 7, 'minors': 11, 'computer': 0, 'time': 4, 'survey': 6, 'interface': 1, 'trees': 9, 'human': 2, 'user': 3, 'graph': 10, 'eps': 8}\n"
     ]
    }
   ],
   "source": [
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (2, 1)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 1), (1, 1)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> new_doc = \"Human computer interaction\"\n",
    ">>> new_vec = dictionary.doc2bow(new_doc.lower().split())\n",
    ">>> print(new_vec)  # the word \"interaction\" does not appear in the dictionary and is ignored\n",
    "[(0, 1), (1, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-13 14:58:54,708 : INFO : storing corpus in Matrix Market format to /tmp/deerwester.mm\n",
      "2017-04-13 14:58:54,710 : INFO : saving sparse matrix to /tmp/deerwester.mm\n",
      "2017-04-13 14:58:54,712 : INFO : PROGRESS: saving document #0\n",
      "2017-04-13 14:58:54,713 : INFO : saved 9x12 matrix, density=25.926% (28/108)\n",
      "2017-04-13 14:58:54,715 : INFO : saving MmCorpus index to /tmp/deerwester.mm.index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1)], [(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)], [(1, 1), (3, 1), (7, 1), (8, 1)], [(2, 1), (7, 2), (8, 1)], [(3, 1), (4, 1), (5, 1)], [(9, 1)], [(9, 1), (10, 1)], [(9, 1), (10, 1), (11, 1)], [(6, 1), (10, 1), (11, 1)]]\n"
     ]
    }
   ],
   "source": [
    ">>> corpus = [dictionary.doc2bow(text) for text in texts]\n",
    ">>> corpora.MmCorpus.serialize('/tmp/deerwester.mm', corpus)  # store to disk, for later use\n",
    ">>> print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus Streaming – One Document at a Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyCorpus(object):\n",
    "    def __iter__(self):\n",
    "        for line in open('mycorpus.txt'):\n",
    "         # assume there's one document per line, tokens separated by whitespace\n",
    "              yield dictionary.doc2bow(line.lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.MyCorpus object at 0x7f0f558f92e8>\n"
     ]
    }
   ],
   "source": [
    "corpus_memory_friendly = MyCorpus()  # doesn't load the corpus into memory!\n",
    "print(corpus_memory_friendly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
